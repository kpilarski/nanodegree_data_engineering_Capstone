{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Immigration Data Warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "##### Ad. Scope\n",
    "The project I have created is to expand U.S. immigration data with additional dimensions. This will allow for wider possibilities when analyzing these data by analysts. In the project I used a star schema with one fact table. For cleaning data, manipulating and creating structure of data warehouse I used Spark, because this technology allow with ease to operations on large data sets. One more thing follows from the use of this technology - data warehouse is saved in parquet file. The use case for this analytical database is to look at this data through the prism of different dimensions and their connections - for example we can find out if there is connection between immigration and demographic or temperature data. It meanse that this data warehouse will allow us to see things like correlation between big aglomerations and immigration, or between warmer places and immigration.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "##### Ad. Describe and Gather Data \n",
    "immigration (for fact table) data comes from the US National Tourism and Trade Office and it is obtained in SAS7BDAT. The temperature data comes from Kaggle and includes temperatures in cities and countries around the world. Next we have Demographic data that comes from OpenSoft. We can find there infomration like male population, female population, median age, race etc. \n",
    "There is also dataset with airport codes. There are also some data coming from I94_SAS_Labels_Descriptions.SAS  - several dimension tables were created from information from this file (for example visa infomration, countries, states or modes (sea/land/air)\n",
    "\n",
    "Description of the most important data sets:\n",
    "\n",
    "immigration - Report contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries). Data sources include: Overseas DHS/CBP I-94 Program data; Canadian visitation data (Stats Canada) and Mexican visitation data (Banco de Mexico).\n",
    "\n",
    "demographic - This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey.\n",
    "\n",
    "temperature - This dataset came from Kaggle and contains information about temperature. More info in: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "\n",
    "airports - This is a simple table of airport codes and corresponding cities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Important!\n",
    "#### We can simply run whole ETL Process in console with following command:\n",
    "`python etl.py`\n",
    "##### The same process, but broken down into separate steps, is here in Jupyter Notebook\n",
    "##### The process launched in the console also points to the most important steps\n",
    "##### We can see preview of this below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![log_ETL_process](static_files/log_capstone.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### In the first step, we import all the data that is immediately available from the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# All necessary path to the files are listed here\n",
    "\n",
    "sas_label_descriptions_path = \"csv_data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "immigration_path = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "temperature_path = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "demographics_path = 'csv_data/us-cities-demographics.csv'\n",
    "airports_path = 'csv_data/airport-codes_csv.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_spark = spark.read.format('com.github.saurfang.sas.spark').load(immigration_path)\n",
    "demographics_spark = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", ';').load(demographics_path)\n",
    "temperature_spark = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", ',').load(temperature_path)\n",
    "airports_spark = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", ',').load(airports_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Let's show raw imported data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "| 18.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MI|20555.0|  57.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247103803E10|00602|      B1|\n",
      "| 19.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  63.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1953.0|09302016|  null|  null|     AZ|9.247139923E10|00602|      B2|\n",
      "| 20.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  57.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247161383E10|00602|      B2|\n",
      "| 21.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20553.0|  46.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1970.0|09302016|  null|  null|     AZ|9.247079603E10|00602|      B2|\n",
      "| 22.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20562.0|  48.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1968.0|09302016|  null|  null|     AZ|9.247848973E10|00608|      B1|\n",
      "| 23.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20671.0|  52.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1964.0|09302016|  null|  null|     TK|9.250139443E10|00001|      B2|\n",
      "| 24.0|2016.0|   4.0| 101.0| 101.0|    TOR|20545.0|    1.0|     MO|20554.0|  33.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1983.0|09302016|  null|  null|     MQ|9.249090503E10|03348|      B2|\n",
      "| 27.0|2016.0|   4.0| 101.0| 101.0|    BOS|20545.0|    1.0|     MA|20549.0|  58.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1958.0|04062016|     M|  null|     LH|9.247876383E10|00422|      B1|\n",
      "| 28.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20549.0|  56.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1960.0|04062016|     F|  null|     LH|9.247890033E10|00422|      B1|\n",
      "| 29.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20561.0|  62.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1954.0|09302016|     M|  null|     AZ|9.250378143E10|00614|      B2|\n",
      "| 30.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NJ|20578.0|  49.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1967.0|09302016|     M|  null|     OS|9.247020943E10|00089|      B2|\n",
      "| 31.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NY|20611.0|  43.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1973.0|09302016|     M|  null|     OS|9.247128923E10|00089|      B2|\n",
      "| 33.0|2016.0|   4.0| 101.0| 101.0|    HOU|20545.0|    1.0|     TX|20554.0|  53.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1963.0|09302016|     F|  null|     TK|9.250930163E10|00033|      B2|\n",
      "| 34.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  48.0|    2.0|  1.0|20160401|     TIA| null|      G|   null|   null|   null| 1968.0|09302016|     M|  null|     AZ|9.247042023E10|00602|      B2|\n",
      "| 35.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  74.0|    2.0|  1.0|20160401|     TIA| null|      T|   null|   null|   null| 1942.0|09302016|     F|  null|     TK|  6.69712185E8|    1|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791|\n",
      "|      Fort Myers|       Florida|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|               White| 50169|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|                  2.13|        PA|               White|208863|\n",
      "|          Laredo|         Texas|      28.8|         124305|           131484|          255789|              4921|       68427|                  3.66|        TX|American Indian a...|  1253|\n",
      "|        Berkeley|    California|      32.5|          60142|            60829|          120971|              3736|       25000|                  2.35|        CA|               Asian| 27089|\n",
      "|     Santa Clara|    California|      35.2|          63278|            62938|          126216|              4426|       52281|                  2.75|        CA|               White| 55847|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt| AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|              6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01| 5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|             10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01| 14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|             16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-08-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-09-01| 12.780999999999999|                        1.454|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-10-01|               7.95|                         1.63|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-11-01|  4.638999999999999|           1.3019999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-12-01|0.12199999999999987|                        1.756|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-01-01|-1.3330000000000002|                        1.642|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-02-01|             -2.732|                        1.358|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-03-01|              0.129|                        1.088|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-04-01|              4.042|                        1.138|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-05-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-06-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|     US-OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|     US-AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|     US-CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|     US-CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|     US-CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "| 00CO|       closed|          Cass Field|        4830|       NA|         US|     US-CO|  Briggsdale|    null|     null|      null|-104.344002, 40.6...|\n",
      "| 00FA|small_airport| Grass Patch Airport|          53|       NA|         US|     US-FL|    Bushnell|    00FA|     null|      00FA|-82.2190017700195...|\n",
      "| 00FD|     heliport|  Ringhaver Heliport|          25|       NA|         US|     US-FL|   Riverview|    00FD|     null|      00FD|-82.3453979492187...|\n",
      "| 00FL|small_airport|   River Oak Airport|          35|       NA|         US|     US-FL|  Okeechobee|    00FL|     null|      00FL|-80.9692001342773...|\n",
      "| 00GA|small_airport|    Lt World Airport|         700|       NA|         US|     US-GA|    Lithonia|    00GA|     null|      00GA|-84.0682983398437...|\n",
      "| 00GE|     heliport|    Caffrey Heliport|         957|       NA|         US|     US-GA|       Hiram|    00GE|     null|      00GE|-84.7339019775390...|\n",
      "| 00HI|     heliport|  Kaupulehu Heliport|          43|       NA|         US|     US-HI| Kailua/Kona|    00HI|     null|      00HI|-155.980233, 19.8...|\n",
      "| 00ID|small_airport|Delta Shores Airport|        2064|       NA|         US|     US-ID|  Clark Fork|    00ID|     null|      00ID|-116.213996887207...|\n",
      "| 00IG|small_airport|       Goltl Airport|        3359|       NA|         US|     US-KS|    McDonald|    00IG|     null|      00IG|-101.395994, 39.7...|\n",
      "| 00II|     heliport|Bailey Generation...|         600|       NA|         US|     US-IN|  Chesterton|    00II|     null|      00II|-87.122802734375,...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Firstly we will create dimension table with countries and corresponding country codes. Below code will extract this information from I94_SAS_Labels_Descriptions.SAS file. There are a lot of invalid codes, so we will takie care of that and take only valid ones. We also are creating list with only valid country codes - we will need those codes later. Below code is creating spark data frame with appropriate structure for data warehouse and also list with valid country codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"country_code\", StringType(), False),\n",
    "StructField(\"country_name\", StringType(), False)])\n",
    "\n",
    "with open(sas_label_descriptions_path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "valid_country_codes_raw = []\n",
    "valid_country_codes_list = []\n",
    "\n",
    "for index, line in enumerate(lines[9:245]):\n",
    "    valid_country_codes_list.append(int(line.split('=')[0].strip()))\n",
    "    valid_country_codes_raw.append({\n",
    "        'country_code' : int(line.split('=')[0].strip()),\n",
    "        'country_name' : line.split('=')[1].strip().replace('\\n','').replace(\"'\",\"\")\n",
    "    })\n",
    "valid_country_codes_raw[0]['country_name'] = \"MEXICO\"\n",
    "\n",
    "countries_dimension = spark.createDataFrame(pd.DataFrame(valid_country_codes_raw), schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|country_code|   country_name|\n",
      "+------------+---------------+\n",
      "|         582|         MEXICO|\n",
      "|         236|    AFGHANISTAN|\n",
      "|         101|        ALBANIA|\n",
      "|         316|        ALGERIA|\n",
      "|         102|        ANDORRA|\n",
      "|         324|         ANGOLA|\n",
      "|         529|       ANGUILLA|\n",
      "|         518|ANTIGUA-BARBUDA|\n",
      "|         687|     ARGENTINA |\n",
      "|         151|        ARMENIA|\n",
      "|         532|          ARUBA|\n",
      "|         438|      AUSTRALIA|\n",
      "|         103|        AUSTRIA|\n",
      "|         152|     AZERBAIJAN|\n",
      "|         512|        BAHAMAS|\n",
      "|         298|        BAHRAIN|\n",
      "|         274|     BANGLADESH|\n",
      "|         513|       BARBADOS|\n",
      "|         104|        BELGIUM|\n",
      "|         581|         BELIZE|\n",
      "+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_dimension.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Secondly we will create dimension table with port codes and corresponding city/area names. Below code will extract this information from I94_SAS_Labels_Descriptions.SAS file. There are a lot of invalid codes, so we will takie care of that and take only valid ones. We also are creating list with only valid port codes - we will need those codes later. Below code is creating spark data frame with appropriate structure for data warehouse and also list with valid port codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"port_code\", StringType(), False),\n",
    "StructField(\"port_city\", StringType(), False)])\n",
    "\n",
    "with open(sas_label_descriptions_path) as f:\n",
    "    content = f.readlines()\n",
    "content = [x.strip() for x in content]\n",
    "ports = content[302:962]\n",
    "splitted_ports = [port.split(\"=\") for port in ports]\n",
    "port_codes = [x[0].replace(\"'\",\"\").strip() for x in splitted_ports]\n",
    "port_locations = [x[1].replace(\"'\",\"\").strip() for x in splitted_ports]\n",
    "port_cities = [x.split(\",\")[0] for x in port_locations]\n",
    "port_states = [x.split(\",\")[-1] for x in port_locations]\n",
    "valid_port_codes_raw = pd.DataFrame({\"port_code\" : port_codes, \"port_city\": port_cities, \"port_state\": port_states})\n",
    "valid_port_codes_raw = valid_port_codes_raw[~valid_port_codes_raw['port_city'].str.contains('No PORT Code')]\n",
    "valid_port_codes_raw = valid_port_codes_raw[~valid_port_codes_raw['port_city'].str.contains('Collapsed')]\n",
    "irregular_ports_df = valid_port_codes_raw[valid_port_codes_raw[\"port_city\"] == valid_port_codes_raw[\"port_state\"]]\n",
    "valid_port_codes_raw = valid_port_codes_raw[~valid_port_codes_raw['port_code'].isin(irregular_ports_df.port_code.tolist())]\n",
    "valid_port_codes_list = pd.DataFrame(valid_port_codes_raw)['port_code'].tolist()\n",
    "\n",
    "ports_dimension = spark.createDataFrame(pd.DataFrame(valid_port_codes_raw)[['port_code', 'port_city']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|port_code|           port_city|\n",
      "+---------+--------------------+\n",
      "|      ALC|               ALCAN|\n",
      "|      ANC|           ANCHORAGE|\n",
      "|      BAR|BAKER AAF - BAKER...|\n",
      "|      DAC|       DALTONS CACHE|\n",
      "|      PIZ|DEW STATION PT LA...|\n",
      "|      DTH|        DUTCH HARBOR|\n",
      "|      EGL|               EAGLE|\n",
      "|      FRB|           FAIRBANKS|\n",
      "|      HOM|               HOMER|\n",
      "|      HYD|               HYDER|\n",
      "|      JUN|              JUNEAU|\n",
      "|      5KE|           KETCHIKAN|\n",
      "|      KET|           KETCHIKAN|\n",
      "|      MOS|MOSES POINT INTER...|\n",
      "|      NIK|             NIKISKI|\n",
      "|      NOM|                 NOM|\n",
      "|      PKC|         POKER CREEK|\n",
      "|      ORI|      PORT LIONS SPB|\n",
      "|      SKA|             SKAGWAY|\n",
      "|      SNP|     ST. PAUL ISLAND|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ports_dimension.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Thirdly we will create dimension table with mode codes and corresponding mode names. By mode name I mean \"mode of movement\"  so air/sea/land/not reported.  Below code will extract this information from I94_SAS_Labels_Descriptions.SAS file.We also are creating list with only valid mode codes. Below code is creating spark data frame with appropriate structure for data warehouse and also list with valid mode codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"mode_id\", IntegerType(), False),\n",
    "StructField(\"mode_name\", StringType(), False)])\n",
    "\n",
    "with open(sas_label_descriptions_path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "valid_mode_codes_raw = []\n",
    "valid_mode_codes_list = []\n",
    "\n",
    "for index, line in enumerate(lines[972:976]):\n",
    "    #print(index, line.split('='))\n",
    "    valid_mode_codes_list.append(int(line.split('=')[0].strip()))\n",
    "    valid_mode_codes_raw.append({\n",
    "        'mode_id':int(line.split('=')[0].strip()),\n",
    "        'mode_name':line.split('=')[1].replace(';','').replace('\\n','').replace(\"'\",\"\").strip()\n",
    "    })\n",
    "\n",
    "modes_dimension = spark.createDataFrame(pd.DataFrame(valid_mode_codes_raw)[['mode_id','mode_name']], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|mode_id|   mode_name|\n",
      "+-------+------------+\n",
      "|      1|         Air|\n",
      "|      2|         Sea|\n",
      "|      3|        Land|\n",
      "|      9|Not reported|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modes_dimension.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Fourthly we will create dimension table with state codes and corresponding state names. Below code will extract this information from I94_SAS_Labels_Descriptions.SAS file. We also are creating list with only valid state codes - we will need those codes later. Below code is creating spark data frame with appropriate structure for data warehouse and also list with valid state codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"state_code\", StringType(), False),\n",
    "StructField(\"state_name\", StringType(), False)])\n",
    "\n",
    "with open(sas_label_descriptions_path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "valid_state_codes_raw = []\n",
    "valid_state_codes_list = []\n",
    "\n",
    "for index, line in enumerate(lines[981:1036]):\n",
    "    #print(index, line.split('='))\n",
    "    valid_state_codes_list.append(line.split('=')[0].strip().replace('\\t','').replace(\"'\",\"\"))\n",
    "    valid_state_codes_raw.append({\n",
    "        'state_code': line.split('=')[0].strip().replace('\\t','').replace(\"'\",\"\") ,\n",
    "        'state_name': line.split('=')[1].replace('\\n','').replace(\"'\",\"\").strip()\n",
    "    }) \n",
    "\n",
    "states_dimension =  spark.createDataFrame(pd.DataFrame(valid_state_codes_raw), schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|state_code|       state_name|\n",
      "+----------+-----------------+\n",
      "|        AL|          ALABAMA|\n",
      "|        AK|           ALASKA|\n",
      "|        AZ|          ARIZONA|\n",
      "|        AR|         ARKANSAS|\n",
      "|        CA|       CALIFORNIA|\n",
      "|        CO|         COLORADO|\n",
      "|        CT|      CONNECTICUT|\n",
      "|        DE|         DELAWARE|\n",
      "|        DC|DIST. OF COLUMBIA|\n",
      "|        FL|          FLORIDA|\n",
      "|        GA|          GEORGIA|\n",
      "|        GU|             GUAM|\n",
      "|        HI|           HAWAII|\n",
      "|        ID|            IDAHO|\n",
      "|        IL|         ILLINOIS|\n",
      "|        IN|          INDIANA|\n",
      "|        IA|             IOWA|\n",
      "|        KS|           KANSAS|\n",
      "|        KY|         KENTUCKY|\n",
      "|        LA|        LOUISIANA|\n",
      "+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states_dimension.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### In this step will create dimension table with visa codes and corresponding visa names. By visa name I mean business/pleasuse or student.  Below code will extract this information from I94_SAS_Labels_Descriptions.SAS file. We also are creating list with only valid visa codes. Below code is creating spark data frame with appropriate structure for data warehouse and also list with valid visa codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"visa_id\", IntegerType(), False),\n",
    "StructField(\"visa_type\", StringType(), False)])\n",
    "\n",
    "with open(sas_label_descriptions_path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "valid_visa_codes_raw = []\n",
    "valid_visa_codes_list = []\n",
    "\n",
    "for index, line in enumerate(lines[1046:1049]):\n",
    "    #print(index, line.split('='))\n",
    "    valid_visa_codes_list.append(int(line.split('=')[0].strip()))\n",
    "    valid_visa_codes_raw.append({\n",
    "        'visa_id': int(line.split('=')[0].strip()),\n",
    "        'visa_type': line.split('=')[1].strip().replace('\\n','').replace(\"'\",\"\") \n",
    "    })\n",
    "\n",
    "visa_dimension = spark.createDataFrame(pd.DataFrame(valid_visa_codes_raw), schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|visa_id|visa_type|\n",
      "+-------+---------+\n",
      "|      1| Business|\n",
      "|      2| Pleasure|\n",
      "|      3|  Student|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa_dimension.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### In this step, we will create our main fact table for immigration. We take into account only the necessary columns, renaming them into appropriate names and setting appropriate data dtypes. We also create a column for the day of immigration, month of immigration and year of immigration (so the arrival). This will be needed because we will partition our table by these columns. At the very end, we also make sure that there are no missing data. We do this by checking whether the data from the appropriate columns is included in the previously prepared lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_spark = immigration_spark.withColumnRenamed(\"i94addr\", \"code_state\") \\\n",
    ".withColumnRenamed(\"i94port\", \"code_port\") \\\n",
    ".withColumn(\"code_visa\", col(\"i94visa\").cast(\"integer\")).drop(\"i94visa\") \\\n",
    ".withColumn(\"code_mode\", col(\"i94mode\").cast(\"integer\")).drop(\"i94mode\") \\\n",
    ".withColumn(\"code_country_origin\", col(\"i94res\").cast(\"integer\")).drop(\"i94res\") \\\n",
    ".withColumn(\"code_country_city\", col(\"i94cit\").cast(\"integer\")).drop(\"i94cit\") \\\n",
    ".withColumn(\"year\", col(\"i94yr\").cast(\"integer\")).drop(\"i94yr\") \\\n",
    ".withColumn(\"month\", col(\"i94mon\").cast(\"integer\")).drop(\"i94mon\") \\\n",
    ".withColumn(\"birth_year\", col(\"biryear\").cast(\"integer\")).drop(\"biryear\") \\\n",
    ".withColumn(\"age\", col(\"i94bir\").cast(\"integer\")).drop(\"i94bir\") \\\n",
    ".withColumn(\"counter_summary\", col(\"count\").cast(\"integer\")).drop(\"count\") \\\n",
    ".withColumn(\"date_base_SAS\", to_date(lit(\"01/01/1960\"), \"MM/dd/yyyy\")) \\\n",
    ".withColumn(\"arrival_date\", expr(\"date_add(date_base_SAS, arrdate)\")) \\\n",
    ".withColumn(\"departure_date\", expr(\"date_add(date_base_SAS, depdate)\")).drop(\"date_base_SAS\", \"arrdate\", \"depdate\")\\\n",
    ".withColumn(\"arrival_date-split\", split(col(\"arrival_date\"), \"-\")) \\\n",
    ".withColumn(\"arrival_year\", col(\"arrival_date-split\")[0].cast(\"integer\")) \\\n",
    ".withColumn(\"arrival_month\", col(\"arrival_date-split\")[1].cast(\"integer\")) \\\n",
    ".withColumn(\"arrival_day\", col(\"arrival_date-split\")[2].cast(\"integer\")) \\\n",
    ".drop(\"arrival_date-split\").drop('dtadfile')\n",
    "\n",
    "\n",
    "immigration_fact = immigration_spark.filter(immigration_spark['code_country_city'].isin(valid_country_codes_list))\\\n",
    ".filter(immigration_spark['code_country_origin'].isin(valid_country_codes_list))\\\n",
    ".filter(immigration_spark['code_port'].isin(valid_port_codes_list))\\\n",
    ".filter(immigration_spark['code_mode'].isin(valid_mode_codes_list))\\\n",
    ".filter(immigration_spark['code_visa'].isin(valid_visa_codes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+---------+---------+-------------------+-----------------+----+-----+----------+---+---------------+------------+--------------+------------+-------------+-----------+\n",
      "|cicid|code_port|code_state|visapost|occup|entdepa|entdepd|entdepu|matflag| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|code_visa|code_mode|code_country_origin|code_country_city|year|month|birth_year|age|counter_summary|arrival_date|departure_date|arrival_year|arrival_month|arrival_day|\n",
      "+-----+---------+----------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+---------+---------+-------------------+-----------------+----+-----+----------+---+---------------+------------+--------------+------------+-------------+-----------+\n",
      "| 16.0|      NYC|        MA|    null| null|      O|      O|   null|      M|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|        2|        1|                101|              101|2016|    4|      1988| 28|              1|  2016-04-01|    2016-04-23|        2016|            4|          1|\n",
      "| 17.0|      NYC|        MA|    null| null|      O|      O|   null|      M|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|        2|        1|                101|              101|2016|    4|      2012|  4|              1|  2016-04-01|    2016-04-23|        2016|            4|          1|\n",
      "| 18.0|      NYC|        MI|    null| null|      O|      O|   null|      M|09302016|  null|  null|     AZ|9.247103803E10|00602|      B1|        1|        1|                101|              101|2016|    4|      1959| 57|              1|  2016-04-01|    2016-04-11|        2016|            4|          1|\n",
      "| 19.0|      NYC|        NJ|    null| null|      O|      K|   null|      M|09302016|  null|  null|     AZ|9.247139923E10|00602|      B2|        2|        1|                101|              101|2016|    4|      1953| 63|              1|  2016-04-01|    2016-04-14|        2016|            4|          1|\n",
      "| 20.0|      NYC|        NJ|    null| null|      O|      K|   null|      M|09302016|  null|  null|     AZ|9.247161383E10|00602|      B2|        2|        1|                101|              101|2016|    4|      1959| 57|              1|  2016-04-01|    2016-04-14|        2016|            4|          1|\n",
      "| 21.0|      NYC|        NY|    null| null|      O|      O|   null|      M|09302016|  null|  null|     AZ|9.247079603E10|00602|      B2|        2|        1|                101|              101|2016|    4|      1970| 46|              1|  2016-04-01|    2016-04-09|        2016|            4|          1|\n",
      "| 22.0|      NYC|        NY|    null| null|      O|      O|   null|      M|09302016|  null|  null|     AZ|9.247848973E10|00608|      B1|        1|        1|                101|              101|2016|    4|      1968| 48|              1|  2016-04-01|    2016-04-18|        2016|            4|          1|\n",
      "| 23.0|      NYC|        NY|    null| null|      O|      O|   null|      M|09302016|  null|  null|     TK|9.250139443E10|00001|      B2|        2|        1|                101|              101|2016|    4|      1964| 52|              1|  2016-04-01|    2016-08-05|        2016|            4|          1|\n",
      "| 24.0|      TOR|        MO|    null| null|      O|      O|   null|      M|09302016|  null|  null|     MQ|9.249090503E10|03348|      B2|        2|        1|                101|              101|2016|    4|      1983| 33|              1|  2016-04-01|    2016-04-10|        2016|            4|          1|\n",
      "| 27.0|      BOS|        MA|     TIA| null|      G|      O|   null|      M|04062016|     M|  null|     LH|9.247876383E10|00422|      B1|        1|        1|                101|              101|2016|    4|      1958| 58|              1|  2016-04-01|    2016-04-05|        2016|            4|          1|\n",
      "| 28.0|      ATL|        MA|     TIA| null|      G|      O|   null|      M|04062016|     F|  null|     LH|9.247890033E10|00422|      B1|        1|        1|                101|              101|2016|    4|      1960| 56|              1|  2016-04-01|    2016-04-05|        2016|            4|          1|\n",
      "| 29.0|      ATL|        MA|     TIA| null|      G|      O|   null|      M|09302016|     M|  null|     AZ|9.250378143E10|00614|      B2|        2|        1|                101|              101|2016|    4|      1954| 62|              1|  2016-04-01|    2016-04-17|        2016|            4|          1|\n",
      "| 30.0|      ATL|        NJ|     TIA| null|      G|      O|   null|      M|09302016|     M|  null|     OS|9.247020943E10|00089|      B2|        2|        1|                101|              101|2016|    4|      1967| 49|              1|  2016-04-01|    2016-05-04|        2016|            4|          1|\n",
      "| 31.0|      ATL|        NY|     TIA| null|      G|      O|   null|      M|09302016|     M|  null|     OS|9.247128923E10|00089|      B2|        2|        1|                101|              101|2016|    4|      1973| 43|              1|  2016-04-01|    2016-06-06|        2016|            4|          1|\n",
      "| 33.0|      HOU|        TX|     TIA| null|      G|      O|   null|      M|09302016|     F|  null|     TK|9.250930163E10|00033|      B2|        2|        1|                101|              101|2016|    4|      1963| 53|              1|  2016-04-01|    2016-04-10|        2016|            4|          1|\n",
      "| 34.0|      NYC|        CT|     TIA| null|      G|   null|   null|   null|09302016|     M|  null|     AZ|9.247042023E10|00602|      B2|        2|        1|                101|              101|2016|    4|      1968| 48|              1|  2016-04-01|          null|        2016|            4|          1|\n",
      "| 35.0|      NYC|        CT|     TIA| null|      T|   null|   null|   null|09302016|     F|  null|     TK|  6.69712185E8|    1|      B2|        2|        1|                101|              101|2016|    4|      1942| 74|              1|  2016-04-01|          null|        2016|            4|          1|\n",
      "| 36.0|      NYC|        NJ|     TIA| null|      G|      O|   null|      M|09302016|     M|  null|     TK|9.250625823E10|00001|      B2|        2|        1|                101|              101|2016|    4|      1979| 37|              1|  2016-04-01|    2016-04-17|        2016|            4|          1|\n",
      "| 37.0|      NYC|        NJ|     TIA| null|      G|      O|   null|      M|09302016|     F|  null|     AZ|9.247561783E10|00608|      B2|        2|        1|                101|              101|2016|    4|      1967| 49|              1|  2016-04-01|    2016-04-23|        2016|            4|          1|\n",
      "| 38.0|      NYC|        NY|     TIA| null|      G|      O|   null|      M|09302016|     M|  null|     AZ|9.248609253E10|00608|      B2|        2|        1|                101|              101|2016|    4|      1983| 33|              1|  2016-04-01|    2016-05-01|        2016|            4|          1|\n",
      "+-----+---------+----------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+---------+---------+-------------------+-----------------+----+-----+----------+---+---------------+------------+--------------+------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_fact.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### In this step, we will create dimension table with demographics data. We take into account only the necessary columns, renaming them into appropriate names and setting appropriate data dtypes. With demographics data we are also grouping the data by race. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_spark = demographics_spark.filter(demographics_spark[\"State Code\"].isin(valid_state_codes_list))\n",
    "demographics_dimension = demographics_spark.withColumnRenamed(\"City\", \"city\") \\\n",
    ".withColumnRenamed(\"State\", \"state\") \\\n",
    ".withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    ".withColumnRenamed(\"Male Population\", \"male_population\") \\\n",
    ".withColumnRenamed(\"Female Population\", \"female_population\") \\\n",
    ".withColumnRenamed(\"Total Population\", \"total_population\") \\\n",
    ".withColumnRenamed(\"Number of Veterans\", \"number_of_veterans\") \\\n",
    ".withColumnRenamed(\"Foreign-born\", \"foreign_born\") \\\n",
    ".withColumnRenamed(\"Average Household Size\", \"average_household_size\") \\\n",
    ".withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    ".withColumnRenamed(\"Race\", \"race\") \\\n",
    ".withColumnRenamed(\"Count\", \"count\") \\\n",
    ".groupBy(col(\"city\"), col(\"state\"), col(\"median_age\"), col(\"male_population\")\\\n",
    "         ,col(\"female_population\"), col(\"total_population\"), col(\"number_of_veterans\")\\\n",
    "         ,col(\"foreign_born\"), col(\"average_household_size\"), col(\"state_code\"))\\\n",
    ".pivot(\"race\").agg(sum(\"count\").cast(\"integer\"))\\\n",
    ".withColumnRenamed(\"American Indian and Alaska Native\", \"american_indian_and_alaska_native\") \\\n",
    ".withColumnRenamed(\"Asian\", \"asian\") \\\n",
    ".withColumnRenamed(\"Black or African-American\", \"black_or_african_american\") \\\n",
    ".withColumnRenamed(\"Hispanic or Latino\", \"hispanic_or_atino\") \\\n",
    ".withColumnRenamed(\"White\", \"white\") \\\n",
    ".fillna({\"american_indian_and_alaska_native\": 0,\n",
    "         \"asian\": 0,\n",
    "         \"black_or_african_american\": 0,\n",
    "         \"hispanic_or_atino\": 0,\n",
    "         \"white\": 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### With airports data we make sure that it will only contain validated ports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airports_dimension = airports_spark.filter(airports_spark[\"iata_code\"].isin(valid_port_codes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+------------+---------+-----------+----------+-------------------+--------+---------+----------+--------------------+\n",
      "| ident|          type|                name|elevation_ft|continent|iso_country|iso_region|       municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+------+--------------+--------------------+------------+---------+-----------+----------+-------------------+--------+---------+----------+--------------------+\n",
      "|   57A| seaplane_base|Tokeen Seaplane Base|        null|       NA|         US|     US-AK|             Tokeen|     57A|      TKI|       57A|-133.32699585, 55...|\n",
      "|  89NY| small_airport|     Maxson Airfield|         340|       NA|         US|     US-NY|     Alexandria Bay|    89NY|      AXB|      89NY|-75.90034, 44.312002|\n",
      "|  AGGF| small_airport|Fera/Maringe Airport|        null|       OC|         SB|     SB-IS|        Fera Island|    AGGF|      FRE|      null| 159.576996, -8.1075|\n",
      "|   ANZ| small_airport| Angus Downs Airport|        1724|       OC|         AU|     AU-NT|Angus Downs Station|    null|      ANZ|      null|  132.2748, -25.0325|\n",
      "|AU-BCK|        closed|[Duplicate] Bolwa...|        null|       OC|         AU|    AU-QLD|           Bolwarra|    null|      BCK|      null|144.169006348, -1...|\n",
      "|AU-COB| small_airport|    Coolibah Airport|        null|       OC|         AU|     AU-NT|           Coolibah|    null|      COB|      null|130.9620056152343...|\n",
      "|AU-CRY| small_airport|Carlton Hill Airport|        null|       OC|         AU|     AU-WA|       Carlton Hill|    null|      CRY|      null|128.5339965820312...|\n",
      "|AU-HIG|        closed|[Duplicate] Highb...|        null|       OC|         AU|    AU-QLD|           Highbury|    null|      HIG|      null|143.145996094, -1...|\n",
      "|AU-MNW|        closed|[Duplicate] Macdo...|        null|       OC|         AU|     AU-NT|    Macdonald Downs|    null|      MNW|      null|135.199005127, -2...|\n",
      "|AU-SWB| small_airport|  Shaw River Airport|        null|       OC|         AU|     AU-WA|         Shaw River|    null|      SWB|      null|119.3619995117187...|\n",
      "|   AUS|        closed|Austin Robert Mue...|        null|       NA|         US|     US-TX|               null|    KAUS|      AUS|      null|-97.6997852325, 3...|\n",
      "|  AYEL| small_airport|   Eliptamin Airport|        4825|       OC|         PG|    PG-SAN|          Eliptamin|    AYEL|      EPT|       ELP|   141.6779, -5.0412|\n",
      "|  AYHH| small_airport|    Honinabi Airport|         452|       OC|         PG|    PG-WPD|           Honinabi|    AYHH|      HNN|       HBI|  142.1771, -16.2457|\n",
      "|  AYMD|medium_airport|      Madang Airport|          20|       OC|         PG|    PG-MPM|             Madang|    AYMD|      MAG|      null|145.789001465, -5...|\n",
      "|  AYMO|medium_airport|      Momote Airport|          12|       OC|         PG|    PG-MRL|       Manus Island|    AYMO|      MAS|      null|147.423996, -2.06189|\n",
      "|  AYPY| large_airport|Port Moresby Jack...|         146|       OC|         PG|    PG-NCD|       Port Moresby|    AYPY|      POM|      null|147.2200012207031...|\n",
      "|  AYRV| small_airport|  May River Airstrip|         107|       OC|         PG|    PG-ESW|          May River|    AYRV|      MRH|       MRV|    141.785, -4.3615|\n",
      "|  AYSQ| small_airport|Sepik Plains Airport|         230|       OC|         PG|    PG-ESW|       Sepik Plains|    AYSQ|      SPV|       SPP|   143.6734, -3.8821|\n",
      "|   BEA|        closed|     Bereina Airport|          58|       OC|         PG|    PG-CPM|    Bereina Mission|    null|      BEA|      null|     146.5083, -8.64|\n",
      "|  BINF| small_airport|NorÃ°fjÃ¶rÃ°ur Ai...|          13|       EU|         IS|      IS-7|     NorÃ°fjÃ¶rÃ°ur|    BINF|      NOR|      null|-13.7463998794555...|\n",
      "+------+--------------+--------------------+------------+---------+-----------+----------+-------------------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_dimension.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### In the data with temperature, we cannot aggregate them by date, because they do not contain data for 2016. However, we can find out what the average temperature was in each month in the given city so we will create such dimension. We will start from pandas data frame, becasue it is easier to tranform data there. We will perform some operation and finally we transform pandas data frame to our temperature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"port_code\", StringType(), False),\n",
    "StructField(\"month\", IntegerType(), False),\n",
    "StructField(\"avg_tempertature\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "temperature_pandas = pd.read_csv(temperature_path)\n",
    "temperature_pandas = temperature_pandas[~temperature_pandas['AverageTemperature'].isnull() ]\n",
    "temperature_pandas['datetime'] = pd.to_datetime(temperature_pandas['dt'], format=\"%Y/%m/%d\")\n",
    "temperature_pandas['month'] = temperature_pandas['datetime'].dt.month\n",
    "temperature_pandas = temperature_pandas.groupby(['City','month'])[['AverageTemperature']].mean().reset_index()\n",
    "temperature_pandas['City']=temperature_pandas['City'].apply(lambda x: x.upper())\n",
    "temperature_pandas = temperature_pandas.merge(ports_dimension.toPandas(),how='inner', left_on = 'City', right_on='port_city')[['port_code','month','AverageTemperature']]\n",
    "temperature_pandas = temperature_pandas.rename({'AverageTemperature':'avg_tempertature'}, axis=1)\n",
    "temperature_pandas['avg_tempertature'] = temperature_pandas['avg_tempertature'].round(decimals=2)\n",
    "temperature_dimension = spark.createDataFrame(temperature_pandas, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------------+\n",
      "|port_code|month|avg_tempertature|\n",
      "+---------+-----+----------------+\n",
      "|      ABE|    1|            4.53|\n",
      "|      ABE|    2|            4.33|\n",
      "|      ABE|    3|            4.67|\n",
      "|      ABE|    4|            5.94|\n",
      "|      ABE|    5|            8.21|\n",
      "|      ABE|    6|           10.97|\n",
      "|      ABE|    7|           13.06|\n",
      "|      ABE|    8|           13.37|\n",
      "|      ABE|    9|           11.91|\n",
      "|      ABE|   10|            9.46|\n",
      "|      ABE|   11|            7.31|\n",
      "|      ABE|   12|            5.69|\n",
      "|      AKR|    1|           -3.21|\n",
      "|      CAK|    1|           -3.21|\n",
      "|      AKR|    2|           -2.12|\n",
      "|      CAK|    2|           -2.12|\n",
      "|      AKR|    3|            2.56|\n",
      "|      CAK|    3|            2.56|\n",
      "|      AKR|    4|            8.87|\n",
      "|      CAK|    4|            8.87|\n",
      "+---------+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_dimension.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "I have implemented star schema. It is the typical schema for a Data Warehouse and together with the snowflake model they are the most popular data warehouse schemas. For cleaning data, creating structure of data warehouse and all operations with data frames and files I used Spark. Data Warehouse schema is saved in parquet file. For this project I used Spark because this technology allows you to easily manipulate large files. Despite the fact that the data for this task was not that large yet, I decided that the data size was sufficient to use a Spark. \n",
    "\n",
    "Here is a scheme for my model:\n",
    "![schema_postgres_database](static_files/scheme_capstone.png)\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "Firstly we need to load whole data and transform them to spark data frames:\n",
    "\n",
    "  - load raw data: immigration, temperature, demographics and airports\n",
    "  \n",
    "  \n",
    "Secondly:\n",
    "  - Clean all the data\n",
    "  - Prepare fact table and all the dimension tables\n",
    "  \n",
    "  \n",
    "Thirdly:\n",
    "  - For integrity and consistency we will insert in fact table only items with dimension keys right.\n",
    "  - Save all the tables into parquet files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Saving our fact table into parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_fact = immigration_fact.join(demographics_dimension, immigration_fact[\"code_state\"] == demographics_dimension[\"state_code\"], \"left_semi\") \\\n",
    ".join(airports_dimension, immigration_fact[\"code_port\"] == airports_dimension[\"iata_code\"], \"left_semi\") \\\n",
    ".join(ports_dimension, immigration_fact[\"code_port\"] == ports_dimension[\"port_code\"], \"left_semi\") \\\n",
    ".join(temperature_dimension, (immigration_fact[\"code_port\"] == temperature_dimension[\"port_code\"]) & (immigration_fact[\"arrival_month\"] == temperature_dimension[\"month\"]), \"left_semi\") \\\n",
    ".join(countries_dimension, immigration_fact[\"code_country_origin\"] == countries_dimension[\"country_code\"], \"left_semi\") \\\n",
    ".join(visa_dimension, immigration_fact[\"code_visa\"] == visa_dimension[\"visa_id\"], \"left_semi\") \\\n",
    ".join(modes_dimension, immigration_fact[\"code_mode\"] == modes_dimension[\"mode_id\"], \"left_semi\") \\\n",
    ".join(states_dimension, immigration_fact[\"code_state\"] == states_dimension[\"state_code\"], \"left_semi\")\n",
    "\n",
    "immigration_fact.write.mode('overwrite').partitionBy(\"arrival_year\", \"arrival_month\", \"arrival_day\").parquet(\"./DWH_IMMIGRATION_PARQUET/immigration_fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Saving our dimension tables into parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries_dimension.write.mode('overwrite').parquet(\"./DWH_IMMIGRATION_PARQUET/countries_dimension.parquet\")\n",
    "ports_dimension.write.mode('overwrite').parquet(\"./DWH_IMMIGRATION_PARQUET/ports_dimension.parquet\")\n",
    "modes_dimension.write.mode('overwrite').parquet(\"./DWH_IMMIGRATION_PARQUET/modes_dimension.parquet\")\n",
    "states_dimension.write.mode('overwrite').parquet(\"./DWH_IMMIGRATION_PARQUET/states_dimension.parquet\")\n",
    "visa_dimension.write.mode('overwrite').parquet(\"./DWH_IMMIGRATION_PARQUET/visa_dimension.parquet\")\n",
    "demographics_dimension.write.mode('overwrite').parquet(\"./DWH_IMMIGRATION_PARQUET/demographics_dimension.parquet\")\n",
    "airports_dimension.write.mode('overwrite').parquet(\"./DWH_IMMIGRATION_PARQUET/airports_dimension.parquet\")\n",
    "temperature_dimension.write.mode('overwrite').parquet(\"./DWH_IMMIGRATION_PARQUET/temperature_dimension.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def quality_check_rows(spark, path_parquet, table_name):\n",
    "    \"\"\"\n",
    "    Function for first quality check.\n",
    "    It validates if given table has any rows -\n",
    "    If table has any rows it means that ETL process worked fine\n",
    "    and the rows are in the table - Then the check is passed.\n",
    "    \"\"\"   \n",
    "    \n",
    "    df = spark.read.parquet(path_parquet)\n",
    "    rows = df.count()\n",
    "    \n",
    "    if rows == 0:\n",
    "        print(f\" - Table {table_name} has 0 rows - quality check failed\")\n",
    "    else:\n",
    "        print(f\" - Table {table_name} has {rows} rows - quality check passed\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def quality_check_joins(spark, path_fact, path_dim_to_check, fact_col, dim_col):\n",
    "    \"\"\"\n",
    "    Function for second quality check.\n",
    "    It validates if join between fact and dimension table is working correctly -\n",
    "    for verification -left_anti- is used here.\n",
    "    \"\"\"    \n",
    "    \n",
    "    df = spark.read.parquet(path_fact)\n",
    "    df_to_check = spark.read.parquet(path_dim_to_check)\n",
    "    \n",
    "    common_values = df.select(col(fact_col)).distinct() \\\n",
    "                             .join(df_to_check, df[fact_col] == df_to_check[dim_col], \"left_anti\") \\\n",
    "                             .count()\n",
    "    \n",
    "    if common_values == 0:\n",
    "        print(f\" - Quality check passed - join between those two tables works correctly\")\n",
    "    else:\n",
    "        print(f\" - Quality check failed - join between those two tables works incorrectly\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1) Count check to ensure completeness of country dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Table country_dimension has 236 rows - quality check passed\n"
     ]
    }
   ],
   "source": [
    "quality_check_rows(spark, \"./DWH_IMMIGRATION_PARQUET/countries_dimension.parquet\", 'country_dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2) Count check to ensure completeness of demographic dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Table demographic_dimension has 596 rows - quality check passed\n"
     ]
    }
   ],
   "source": [
    "quality_check_rows(spark, \"./DWH_IMMIGRATION_PARQUET/demographics_dimension.parquet\", 'demographic_dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 3) Count check to ensure completeness of immigration fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Table immigration_fact has 1387262 rows - quality check passed\n"
     ]
    }
   ],
   "source": [
    "quality_check_rows(spark, \"./DWH_IMMIGRATION_PARQUET/immigration_fact.parquet\", 'immigration_fact')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4) Integrity check to ensure if join works properly between immigration fact table and demographic dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Quality check passed - join between those two tables works correctly\n"
     ]
    }
   ],
   "source": [
    "quality_check_joins(spark, path_fact=\"./DWH_IMMIGRATION_PARQUET/immigration_fact.parquet\", path_dim_to_check=\"./DWH_IMMIGRATION_PARQUET/demographics_dimension.parquet\", fact_col='code_state', dim_col='state_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 5) Integrity check to ensure if join works properly between immigration fact table and state dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Quality check passed - join between those two tables works correctly\n"
     ]
    }
   ],
   "source": [
    "quality_check_joins(spark, path_fact=\"./DWH_IMMIGRATION_PARQUET/immigration_fact.parquet\", path_dim_to_check=\"./DWH_IMMIGRATION_PARQUET/states_dimension.parquet\", fact_col='code_state', dim_col='state_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Fact tables:\n",
    "- ___immigration___ - records with immigration data; data comes from I94_SAS_Labels_Descriptions.SAS file after cleaning\n",
    "    - cicid bigint\n",
    "    - code_port string \n",
    "    - code_state string \n",
    "    - visapost string \n",
    "    - occup string \n",
    "    - entdepa string \n",
    "    - entdepd string \n",
    "    - entdepu string \n",
    "    - matflag string \n",
    "    - dtaddto string \n",
    "    - gender string \n",
    "    - insnum string \n",
    "    - airline string \n",
    "    - admnum double \n",
    "    - fltno string \n",
    "    - visatype string \n",
    "    - code_visa integer \n",
    "    - code_mode integer \n",
    "    - code_country_origin integer \n",
    "    - code_country_city integer \n",
    "    - year integer \n",
    "    - month integer \n",
    "    - birth_year integer \n",
    "    - age integer \n",
    "    - counter_summary integer \n",
    "    - arrival_date date \n",
    "    - departure_date date \n",
    "    - arrival_year integer \n",
    "    - arrival_month integer \n",
    "    - arrival_day integer\n",
    "    \n",
    " \n",
    "#### Dimension tables:\n",
    "- ___demographic___ - demographic data; data comes from us-cities-demographics.csv file after cleaning\n",
    "    - state_code string\n",
    "    - city string\n",
    "    - state string\n",
    "    - median_age string\n",
    "    - male_population string\n",
    "    - female_population string\n",
    "    - total_population string\n",
    "    - number_of_veterans string\n",
    "    - foreign_born string\n",
    "    - average_household_size string\n",
    "    - american_indian_and_alaska_native integer\n",
    "    - asian integer\n",
    "    - black_or_african_american integer\n",
    "    - hispanic_or_atino integer\n",
    "    - white integer\n",
    "    \n",
    "   \n",
    "- ___temperature___ - temperature data; data comes from GlobalLandTemperaturesByCity.csv file after cleaning\n",
    "    - port_code string\n",
    "    - month integer\n",
    "    - avg_tempertature double \n",
    "    \n",
    "    \n",
    "- ___airport___ - airports data; data comes from airport-codes_csv.csv file after cleaning\n",
    "    - ident string\n",
    "    - type string\n",
    "    - name string\n",
    "    - elevation_ft string\n",
    "    - continent string\n",
    "    - iso_country string\n",
    "    - iso_region string\n",
    "    - municipality string\n",
    "    - gps_code string\n",
    "    - iata_code string\n",
    "    - local_code string\n",
    "    - coordinates string\n",
    "    \n",
    "    \n",
    "- ___state___ - state codes data; data comes from I94_SAS_Labels_Descriptions.SAS file after cleaning\n",
    "    - state_code string\n",
    "    - state_name string\n",
    "    \n",
    "    \n",
    "- ___country___ - countries data; data comes from I94_SAS_Labels_Descriptions.SAS file after cleaning\n",
    "    - country_code string\n",
    "    - country_name string\n",
    "    \n",
    "    \n",
    "- ___visa___ - visa codes data; data comes from I94_SAS_Labels_Descriptions.SAS file after cleaning\n",
    "    - visa_id integer\n",
    "    - visa_type string\n",
    "    \n",
    "    \n",
    "- ___mode___ - modes data; data comes from I94_SAS_Labels_Descriptions.SAS file after cleaning\n",
    "    - mode_id integer\n",
    "    - mode_name string\n",
    "    \n",
    "    \n",
    "- ___port___ - ports data; data comes from I94_SAS_Labels_Descriptions.SAS file after cleaning\n",
    "    - port_code string\n",
    "    - port_name string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### 1) I have used Apache Spark to do all the cleaning, manipulating data and creating database model. I have chosen this tool because it can handle large amounts of data easily and the Apache Spark project itself is very widely used so there is a lot of information about Apache Spark in the web (Apache Spark also has very good documentation). The data model files are in parquet files so the scalibility of this project is very high.\n",
    "\n",
    "##### 2) As the partition is daily - we should update ourt data every day.\n",
    "\n",
    "\n",
    "##### a) Apache Spark can still be used even if the data will be increased 100x.\n",
    "\n",
    "##### b) My project requires a daily data update. I would use Apache Airflow for this.\n",
    "\n",
    "##### c) If a hundred people would need access to the data I think Apache Hive would be a good choice. It has a low entry barrier and you only need SQL knowledge to navigate well in Hive.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
